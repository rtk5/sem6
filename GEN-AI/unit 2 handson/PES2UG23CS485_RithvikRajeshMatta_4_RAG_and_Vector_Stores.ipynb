{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "411ce1c2"
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "ace94db8"
   },
   "source": [
    "# Unit 2: RAG, Vector Stores, and Indexing\n",
    "\n",
    "## Introduction\n",
    "LLMs have a knowledge cutoff and can hallucinate. **Retrieval Augmented Generation (RAG)** solves this by retrieving relevant data and injecting it into the prompt.\n",
    "\n",
    "In this notebook, we will master:\n",
    "1.  **Embeddings:** Representing text as vectors.\n",
    "2.  **Vector Stores:** Storing and searching vectors (FAISS).\n",
    "3.  **NaÃ¯ve RAG:** The standard Retrieval -> Augment -> Generate pipeline.\n",
    "4.  **Indexing Challenges:** Deep dive into how vector databases search efficiently (Flat, IVF, HNSW, PQ).\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4a: Embeddings & Vector Space\n",
    "\n",
    "### 1. Introduction: Computers Don't Read English\n",
    "\n",
    "If you ask a computer \"Is a cat similar to a dog?\", it doesn't know. To a computer, \"cat\" is just a string of bytes: `01100011...`.\n",
    "\n",
    "To solve this, we use **Embeddings**.\n",
    "\n",
    "### What is an Embedding?\n",
    "An embedding is a translation from **Words** to **Lists of Numbers (Vectors)**, such that similar words represent close numbers.\n",
    "\n",
    "### The Process (Flowchart)\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"Input Text ('Apple')\"] -->|Tokenization| B[\"Tokens (101, 255)\"]\n",
    "    B -->|Embedding Model| C[\"Vector List ([0.1, -0.5, 0.9...])\"]\n",
    "    C -->|Store| D[\"Vector Database\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667,
     "referenced_widgets": [
      "2a008c818bed455bb74e8d02e1d91c21",
      "ac042e40825e45a2b962b898835f49d1",
      "0f06ad8e18e046bf968fd78c16fa0d32",
      "0286770a8cda4b16be09993c66f8967d",
      "9cb002e1129f4c78b6ef4ed865ba4028",
      "ac4780c9fce248e9ab92e667ce935d1c",
      "d4242b140e3046a89e1e9362d072cd47",
      "faca8f81a48447419567ed1172a4e1af",
      "7e1d3c079d174f77917bcb75a0716bed",
      "83237dc7cb874335b450fa92252c7b49",
      "54158694c8b44f8082b60e17bc90cfa0",
      "c31a75c0c7f94a47a4755e3775dee6ec",
      "6cc4b82fa5784552b3c6e0b973e02a6f",
      "cda5904586d440b09dc895a52203c285",
      "c32b673c1ce44f839f71761058454c3f",
      "73472e0df0ae41338c28104d6fba6d29",
      "5ca641b09c4c411f9208fd3e3dda0568",
      "17ccb801f37d4047a3d4a14fb5f343fc",
      "951008e94dc9440fa22216706e817868",
      "c0aa00910dc446189af1b7c6d258bb0c",
      "d6803a1012e443cfbfcc8cfe1fdbee80",
      "f382487d0fff4b418b7c8a8459b119dd",
      "bc098cdf91d440bca51ff52a9f0cecef",
      "07c7ebddf2c347c482a1e82d4676ee6e",
      "5ddaf92245514cfebb8184b106c3df05",
      "8bd743e1a2414e67945ab41efd4c3289",
      "99483f8245454cd1a3079f7e3ef1336d",
      "565a264c263d494d89cef6777a59fde0",
      "6569525278334e8a9f94824f86bd401c",
      "f4adb3a2e2b9404e98683e8c04230452",
      "0b3041bee3fe43ce90b53796938a1a4e",
      "24101c5e5f284e359d5bb512f3851808",
      "a921cbfa14024f9b8242890ae6edcad9",
      "ac5e67e8c669473394e0782d7ca660e2",
      "ec70ba1c4f604b928885d34109850cba",
      "9757bbcf2dd34709965ee0de47e959a9",
      "d6efd4e52b4749be9020ab746912b3e0",
      "17a99eed41dd402096aa3519c82b501a",
      "c0a63e28c50b40d7a7e7b31926fba173",
      "5c8411f4e2ce44718b5194ca9672ad18",
      "0f726ae037eb40c9b49daa63e5966c8f",
      "ced8ec96e921400a9d7c8bea9bbf258e",
      "59fbab43b7c94a2299bf2125db7af535",
      "e1082af169674c8aa78524a38c716d0a",
      "ed76a9f6ab404e7593eec2252d9b792d",
      "6054089dcf2f497d87961d4bfed1ccd5",
      "270191a21ec2450ab965c2b5093b0f26",
      "f7218a4bd32d46f9bb883840d536cf2a",
      "5fcd0ca9bef748b18a1d8c6b1defeb60",
      "ea005dae7c634635bcbe4c352d0ebecc",
      "625639c974394511bd2a26489e8f2206",
      "ba74ae82337649d0b0f083ecbe794ebd",
      "eeae28bda36d4718a268d1e6d0bd536d",
      "8183a6dca46949ceba1e5d888200e39c",
      "087f7b4860004cb58ef9e2cbbdd20a8b",
      "4b6cf4664be142c383df6ca459739ab5",
      "0bee463232674443bac49aea814d4fa6",
      "e65190e7070e427eb162dfa81f0ed889",
      "5a7ccf3fcfaf4cec867a47a52f4e880a",
      "b0194c79a6cd4c8e8e42e850f4375864",
      "8a00fcd44f114d9098d2f5bd8703e62e",
      "6cfba16b954b4b88b34b7e1fb5a9d40f",
      "cf6e31ad85bf432eae10e80db26907a9",
      "d6d52975a7b24ed6878b72c20e66efc9",
      "f617f821c7634e5f85bef276c434ed93",
      "f18b719dc2514d8588a0e85c47fd0cde",
      "91997b537e9e4c36bc09a2d731a67f4f",
      "e69ad361020b4067892513fbf7857818",
      "3bf7b00b33824fb9975a1ae192f94d52",
      "92e7e0271068431dab67c60eb41571c6",
      "ef07db0ae3884bf48d87be83771e1d02",
      "cdc601b9169a417ca1046c2e9004325a",
      "8ae0b3236161441a890845cb1fbae3e9",
      "5a9abdb487634e3997554a4f8a8f1815",
      "484569670bc245bb9880a2b0d7fbca39",
      "0ecf485ed7d0416884c005a4cc878545",
      "e21bebd86a894f62967496b47a9e91e0",
      "25620f434cb64545bf280f1d5944bc55",
      "69520636b4bf4b84910e7d3dcf0242a3",
      "37aec27c8cea41b4a873ba43d5b6b628",
      "dc7cd1474a604462b8383ff531631a71",
      "0684e67c9b4b47c89763c604902d03c4",
      "3fe3a47302fe41b2900820af87e4e434",
      "e4435767c97b4321a8d4b34e3ea52f67",
      "033d3e3ea7ce47b7a62f00363021ac5c",
      "b7f68ad8fd2b4be3a7164f25b5b760ff",
      "62d7750405fe43809c94bf9723a27247",
      "48811e0043ba4b70ad02fb8ac4ed12a4",
      "002bb70f05324f2fbf818d2558583a24",
      "54bd64a63d324b99ac675e4720333da6",
      "a9855ac82eb1492a866c5bc55bb76fea",
      "d2ee35458b1f4036949d91348d871529",
      "9e6bf5e1e5984befb13aea5f07a61852",
      "5f7f0d36d41f4508b0692fa9218fb1e1",
      "3f00e77a144c46a197c4165ae825c12b",
      "9cd20a18442841a29625bd36c652c949",
      "b7d2a1e010a44969899d4a4b77dcd641",
      "2f13bdbdab1846478ac266df0c6acfe3",
      "9c0df775aa1e47578018ec61f8767b65",
      "8a600080e539429398f2fba6e74a7f4c",
      "2f722bb7d88e4c3497771ca04fc35cfc",
      "857836f866994db6984f3e647aaf855d",
      "ef5999aa31124637bf350bce1105b8e4",
      "f97dd06275b443f1afc8354da38e1dc4",
      "1fe5aa1553544399b8166a0e687ff717",
      "0ef5f1d4c1714b2c84b2bb2a72cf51bf",
      "a3580157a9a8430fbda2c1cc9221e411",
      "7ffc6d9c57d0460992583e69c6d224a5",
      "2f7eb2fd94474a8b9a3d89ca75a2e59d",
      "de9308204eac446fb794217d1f33c36f",
      "0aca8a533c414ebc9306e97239837c85",
      "b71d7a09f4df4a3e91c852d3fd357487",
      "d5ef934bff024a0d9cfed2514f3abef4",
      "cb96f512118141f7ad3728752ce22318",
      "5d72880ee2b74d3cb8af471cde0e0ef6",
      "08fbba440be04f77b7c353830690f030",
      "ac8197de543340daa615259dfaef5d1a",
      "c13d5488560c4f4e82fd9c3e3062524b",
      "5ff0ba780dd34dda9c30abb091a12c7d",
      "ef96146f0767436b93e821b40d394daf",
      "8580ceb34d6247fc943930d06c693e1f"
     ]
    },
    "id": "2bd7eb17",
    "outputId": "4194f36b-1ef2-4e12-e7e4-641a8858f6a7"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet langchain langchain-huggingface sentence-transformers langchain-community\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Using a FREE, open-source model from Hugging Face\n",
    "# 'all-MiniLM-L6-v2' is small, fast, and very good for English.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "718acb1f"
   },
   "source": [
    "## 2. Viewing a Vector\n",
    "\n",
    "Let's see what the word \"Apple\" looks like to the machine.\n",
    "\n",
    "### Conceptual Note: Dimensions\n",
    "The vector below has **384 dimensions** (for MiniLM).\n",
    "- Imagine a graph with X and Y axes (2 Dimensions). You can plot a point (x, y).\n",
    "- Now imagine adding Z (3 Dimensions).\n",
    "- Now imagine **384 axes**.\n",
    "\n",
    "Each axis represents a feature (e.g., \"Is it a fruit?\", \"Is it red?\", \"Is it tech-related?\"). The numbers aren't random; they encode meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c67eb1c",
    "outputId": "9e22e3b6-4c47-4e5f-a9b6-f8198a1a0785"
   },
   "outputs": [],
   "source": [
    "vector = embeddings.embed_query(\"Apple\")\n",
    "\n",
    "print(f\"Dimensionality: {len(vector)}\")\n",
    "print(f\"First 5 numbers: {vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "da0fd791"
   },
   "source": [
    "## 3. The Math: Cosine Similarity\n",
    "\n",
    "How do we know if two vectors are close? We measure the **Angle** between them.\n",
    "\n",
    "### Cosine Similarity Formula\n",
    "$$ \\text{Similarity} = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} $$\n",
    "\n",
    "- **1.0**: Arrows point in the Exact Same Direction (Identical).\n",
    "- **0.0**: Arrows are Perpendicular (Unrelated).\n",
    "- **-1.0**: Arrows point in Opposite Directions (Opposite).\n",
    "\n",
    "**Experiment:** Let's compare \"Cat\", \"Dog\", and \"Car\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58dd1396",
    "outputId": "da09b8dd-2f98-4f22-dea0-2ba652545dd5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "vec_cat = embeddings.embed_query(\"Cat\")\n",
    "vec_dog = embeddings.embed_query(\"Dog\")\n",
    "vec_car = embeddings.embed_query(\"Car\")\n",
    "\n",
    "print(f\"Cat vs Dog: {cosine_similarity(vec_cat, vec_dog):.4f}\")\n",
    "print(f\"Cat vs Car: {cosine_similarity(vec_cat, vec_car):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "5a90b5f7"
   },
   "source": [
    "### Analysis\n",
    "You should see that **Cat & Dog** score higher (e.g., ~0.8) than **Cat & Car** (e.g., ~0.3).\n",
    "This Mathematical Distance is the foundation of all Search engines and RAG systems.\n",
    "\n",
    "This is arguably the most important concept in modern AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "f6278ad2"
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "1865ce8f"
   },
   "source": [
    "# Unit 2 - Part 4b: Naive RAG Pipeline\n",
    "\n",
    "## 1. Introduction: The Open-Book Test\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is just an Open-Book Test architecture.\n",
    "1.  **Retrieval:** Find the right page in the textbook.\n",
    "2.  **Generation:** Write the answer using that page.\n",
    "\n",
    "### The Pipeline (Flowchart)\n",
    "```mermaid\n",
    "graph TD\n",
    "    User[User Question] --> Retriever[Retriever System]\n",
    "    Retriever -->|Search Database| Docs[Relevant Documents]\n",
    "    Docs --> Combiner[Prompt Template]\n",
    "    User --> Combiner\n",
    "    Combiner -->|Full Prompt w/ Context| LLM[Gemini Model]\n",
    "    LLM --> Answer[Final Answer]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVekT5GM2WRg",
    "outputId": "01c9d3e0-6da5-4080-aed9-255ee5ff32b0"
   },
   "outputs": [],
   "source": [
    "pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dcd7e45",
    "outputId": "6b457de2-afb0-4abd-b3ba-5b345f501633"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%pip install python-dotenv --upgrade --quiet faiss-cpu langchain-huggingface sentence-transformers langchain-community\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# Using the same free model as Part 4a\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "b6f457ce"
   },
   "source": [
    "## 2. The \"Knowledge Base\" (Grounding)\n",
    "\n",
    "LLMs hallucinate because they rely on \"parametric memory\" (what they learned during training).\n",
    "RAG introduces \"non-parametric memory\" (external facts).\n",
    "\n",
    "Let's define some facts the LLM definitely *does not* know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "234faff9"
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"Piyush's favorite food is Pizza with extra cheese.\"),\n",
    "    Document(page_content=\"The secret password to the lab is 'Blueberry'.\"),\n",
    "    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "2c5812c6"
   },
   "source": [
    "## 3. Indexing ( Storing the knowledge)\n",
    "\n",
    "We use **FAISS** (Facebook AI Similarity Search) to store the embeddings.\n",
    "Think of FAISS as a super-fast librarian that organizes books by content, not title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "c1e1581d"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "d5d826cf"
   },
   "source": [
    "## 4. The RAG Chain\n",
    "\n",
    "We use LCEL to stitch it together.\n",
    "\n",
    "**Step 1:** The `retriever` takes the question, converts it to numbers, and finds the closest document.\n",
    "**Step 2:** `RunnablePassthrough` holds the question.\n",
    "**Step 3:** The `prompt` combines them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67a7a5ab",
    "outputId": "e9fdffc3-ec8d-4729-b6ee-083047b8f526"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"\n",
    "Answer based ONLY on the context below:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"What is the secret password?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "d4c34d42"
   },
   "source": [
    "## 5. Analysis\n",
    "\n",
    "The retrieval step is opaque here. In the next notebook (**4c**), we will look *inside* the retriever to understand how FAISS actually finds that document among millions of others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "b988faa1"
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "764e4fe9"
   },
   "source": [
    "# Unit 2 - Part 4c: Deep Dive into Indexing Algorithms\n",
    "\n",
    "## 1. Introduction: The Scale Problem\n",
    "\n",
    "Comparing 1 vector against 10 vectors is fast.\n",
    "Comparing 1 vector against **100 Million** vectors is slow.\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)** was built to solve this.\n",
    "\n",
    "### The Trade-off Triangle\n",
    "You can pick 2:\n",
    "- **Speed** (Query time)\n",
    "- **Accuracy** (Recall)\n",
    "- **Memory** (RAM usage)\n",
    "\n",
    "We will explore algorithms that optimize different corners of this triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "cc999db8"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Mock Data: 10,000 vectors of size 128\n",
    "d = 128\n",
    "nb = 10000\n",
    "xb = np.random.random((nb, d)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "f0c4b504"
   },
   "source": [
    "## 2. Flat Index (Brute Force)\n",
    "\n",
    "**Concept:** Check every single item.\n",
    "\n",
    "- **Algo:** `IndexFlatL2`\n",
    "- **Pros:** 100% Accuracy (Gold Standard).\n",
    "- **Cons:** Slow (O(N)). Unusable at 1M+ vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "847ac87f",
    "outputId": "f7e4feb9-67ec-4f25-940d-fcf701983447"
   },
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(xb)\n",
    "print(f\"Flat Index contains {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "507e1ffc"
   },
   "source": [
    "## 3. IVF (Inverted File Index)\n",
    "\n",
    "**Concept:** Clustering / Partitioning.\n",
    "\n",
    "Imagine looking for a book. Instead of checking every shelf, you go to the \"Sci-Fi\" section. Then you only search books *in that section*.\n",
    "\n",
    "### How it works (Flowchart)\n",
    "```mermaid\n",
    "graph TD\n",
    "    Data[All 1M Vectors] -->|Train| Clusters[1000 Cluster Centers (Centroids)]\n",
    "    Query[User Query] -->|Step 1| FindClosest[Find Closest Centroid]\n",
    "    FindClosest -->|Step 2| Search[Search ONLY vectors in that Cluster]\n",
    "```\n",
    "\n",
    "**Analogy:** Voronoi Cells (Zip Codes). We only search the local zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "402817e5"
   },
   "outputs": [],
   "source": [
    "nlist = 100 # How many 'zip codes' (clusters) we want\n",
    "quantizer = faiss.IndexFlatL2(d) # The calculator for distance\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
    "\n",
    "# We MUST train it first so it learns where the clusters are\n",
    "index_ivf.train(xb)\n",
    "index_ivf.add(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "4c0a28c5"
   },
   "source": [
    "## 4. HNSW (Hierarchical Navigable Small World)\n",
    "\n",
    "**Concept:** Six Degrees of Separation.\n",
    "\n",
    "Most data is connected. HNSW builds a **Graph**.\n",
    "- **Layer 0:** Every point connects to neighbors.\n",
    "- **Layer 1:** \"Express Highways\" connecting distant points.\n",
    "\n",
    "**Analogy:** Catching a flight.\n",
    "You don't fly Local -> Local -> Local.\n",
    "You fly Local -> **HUB** (Chicago) -> **HUB** (London) -> Local.\n",
    "\n",
    "- **Pros:** Extremely fast retrieval.\n",
    "- **Cons:** Heavier on RAM (needs to store the edges of the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "42c18025"
   },
   "outputs": [],
   "source": [
    "M = 16 # Number of connections per node (The 'Hub' factor)\n",
    "index_hnsw = faiss.IndexHNSWFlat(d, M)\n",
    "index_hnsw.add(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "645d63c1"
   },
   "source": [
    "## 5. PQ (Product Quantization)\n",
    "\n",
    "**Concept:** Compression (Lossy).\n",
    "\n",
    "Do we need 32-bit float precision (`0.123456789`)? No. `0.12` is fine.\n",
    "PQ breaks the vector into chunks and approximates them.\n",
    "\n",
    "**Analogy:** 4K Video vs 480p Video.\n",
    "- 480p is blurry, but it's 10x smaller and faster to stream.\n",
    "- Use PQ when you are RAM constrained (e.g., storing 1 Billion vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "598642ac",
    "outputId": "04a2c179-1de3-4fab-dd87-7cb47d95226c"
   },
   "outputs": [],
   "source": [
    "m = 8 # Split vector into 8 sub-vectors\n",
    "index_pq = faiss.IndexPQ(d, m, 8)\n",
    "index_pq.train(xb)\n",
    "index_pq.add(xb)\n",
    "print(\"PQ Compression complete. RAM usage minimized.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
