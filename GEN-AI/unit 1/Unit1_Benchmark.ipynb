{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rHMX-8uy3giq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch --quiet\n",
        "from transformers import pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis\n",
        "Encoder-only models (BERT, RoBERTa) should fail or behave oddly because they are not\n",
        "trained for auto-regressive text generation.\n",
        "BART should succeed because it has a decoder trained for generation.\n"
      ],
      "metadata": {
        "id": "V7UlbabF4IiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n",
        "\n",
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name} OUTPUT:\")\n",
        "    try:\n",
        "        generator = pipeline(\"text-generation\", model=model)\n",
        "        output = generator(prompt, max_length=30, num_return_sequences=1)\n",
        "        print(output[0][\"generated_text\"])\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIvR7dlr4GQR",
        "outputId": "1250ad2f-a6b6-426b-d0cf-6c2c5e1668c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT OUTPUT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n",
            "\n",
            "RoBERTa OUTPUT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is\n",
            "\n",
            "BART OUTPUT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is outage CounselSalt When advised adore musSalt coloring Search spoof biggesttext spoof rhythm crowatin crow crow morp blacks DEL DEL DELffe advertisements adore spoof crow crow 720 crow lit adore adore adorerequires cementantis DEL crow crowEducation adore adoreと DEL DEL adore DEL denounce denounce adore cement organizational adore spoofCOR adore organizational mating adoresponsored crow crow adore crow adore adoretextou Americantis adore adore biggest biggesttext Sinn adore adoreparable drawer compos adoretext adore adore organizational DELvered adore adore DEL DEL biggest biggest Demons adoreLuckily adore adoreicrobial adore adore cement adore DEL biggest drawerparable biggest biggest drawer DELantisparable adore adoreRegarding adore drawer illust adore illusttext adoreと adore adore drawer drawer crow drawertexttext drawer drawerparableparableparableicrobialparable biggest adore adore biomass drawer Tirtext adore drawer DELtextantis drawer adoretextparable adoreparableantisparable drawer adore drawer adore adore elders adore crow biggest adore drawerTRY adoretext illust drawer illust illust Counsel drawerparabletextparable drawer littextetermin drawertext drawerparableeterminparabletext adoretext DEL drawer drawer drawer illusttext SearchLuckilytext drawertextparabletexttextparableparableVS drawerparableantistext biggesttext Searchtextou SearchLuckily drawertext Distribut drawer drawer compostexttext crow drawer Search drawer drawertext illustparableparabletextantis illust crowparable drawer drawer biggesttexttext illust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask_models = {\n",
        "    \"BERT\": (\"bert-base-uncased\", \"[MASK]\"),\n",
        "    \"RoBERTa\": (\"roberta-base\", \"<mask>\"),\n",
        "    \"BART\": (\"facebook/bart-base\", \"<mask>\")\n",
        "}\n",
        "\n",
        "for name, (model, mask_token) in fill_mask_models.items():\n",
        "    print(f\"\\n{name} OUTPUT:\")\n",
        "    pipe = pipeline(\"fill-mask\", model=model)\n",
        "    sentence = f\"The goal of Generative AI is to {mask_token} new content.\"\n",
        "    results = pipe(sentence)\n",
        "    for r in results[:3]:\n",
        "        print(r[\"token_str\"], \" | score:\", round(r[\"score\"], 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVpJMTyF4GMr",
        "outputId": "bcb5c464-7f2a-4ac9-dce6-e0a667d7435f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT OUTPUT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create  | score: 0.5397\n",
            "generate  | score: 0.1558\n",
            "produce  | score: 0.0541\n",
            "\n",
            "RoBERTa OUTPUT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " generate  | score: 0.3711\n",
            " create  | score: 0.3677\n",
            " discover  | score: 0.0835\n",
            "\n",
            "BART OUTPUT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " create  | score: 0.0746\n",
            " help  | score: 0.0657\n",
            " provide  | score: 0.0609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_models = {\n",
        "    \"BERT\": \"bert-base-uncased\",\n",
        "    \"RoBERTa\": \"roberta-base\",\n",
        "    \"BART\": \"facebook/bart-base\"\n",
        "}\n",
        "\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "for name, model in qa_models.items():\n",
        "    print(f\"\\n{name} OUTPUT:\")\n",
        "    qa = pipeline(\"question-answering\", model=model)\n",
        "    result = qa(question=question, context=context)\n",
        "    print(\"Answer:\", result[\"answer\"])\n",
        "    print(\"Score:\", round(result[\"score\"], 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTyCqPeu4GJ-",
        "outputId": "983f7fc6-5403-4773-a847-223aea37fdaa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT OUTPUT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: hallucinations, bias, and deepfakes\n",
            "Score: 0.0085\n",
            "\n",
            "RoBERTa OUTPUT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: risks such as hallucinations, bias, and deepfakes\n",
            "Score: 0.0081\n",
            "\n",
            "BART OUTPUT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Generative\n",
            "Score: 0.0078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Generation** | BERT | *Failure* | The model repeated punctuation and failed to generate a meaningful continuation beyond the prompt. | BERT is an encoder-only model trained for understanding and masked token prediction, not auto-regressive text generation. |\n",
        "|  | RoBERTa | *Failure* | The model echoed the prompt without generating any additional content. | RoBERTa is also encoder-only and lacks a decoder to generate new tokens sequentially. |\n",
        "|  | BART | *Partial Success* | The model generated a long sequence of text, but the output was incoherent and largely gibberish. | Although BART has a decoder, it is trained for sequence-to-sequence tasks, not causal language modeling used in free text generation. |\n",
        "| **Fill-Mask** | BERT | *Success* | The model confidently predicted appropriate words such as \"create\", \"generate\", and \"produce\". | BERT was explicitly trained using Masked Language Modeling (MLM), making it well-suited for predicting missing words. |\n",
        "|  | RoBERTa | *Success* | The model correctly predicted words like \"generate\" and \"create\" with high probability scores. | RoBERTa uses an optimized MLM training strategy, improving its ability to predict masked tokens. |\n",
        "|  | BART | *Partial Success* | The model predicted reasonable words such as \"create\", but with much lower confidence scores. | BART is trained using denoising objectives rather than pure MLM, so masked token prediction is not its primary strength. |\n",
        "| **QA** | BERT | *Partial Success* | The model extracted the correct answer span but with a very low confidence score. | While BERT’s encoder architecture supports extractive QA, it was not fine-tuned on QA datasets like SQuAD. |\n",
        "|  | RoBERTa | *Partial Success* | The model returned a more complete answer span but still showed very low confidence. | RoBERTa provides strong contextual representations but lacks a trained question-answering head. |\n",
        "|  | BART | *Failure* | The model returned an incorrect and unrelated answer (\"Generative\"). | BART is not designed for extractive question answering and requires task-specific fine-tuning to perform QA effectively. |\n"
      ],
      "metadata": {
        "id": "yOHi7Dmq5s4z"
      }
    }
  ]
}